{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.00250272\n",
      "Iteration 2, loss = 3.40553964\n",
      "Iteration 3, loss = 2.83501057\n",
      "Iteration 4, loss = 2.31506032\n",
      "Iteration 5, loss = 1.83550853\n",
      "Iteration 6, loss = 1.51090872\n",
      "Iteration 7, loss = 1.32850876\n",
      "Iteration 8, loss = 1.24046965\n",
      "Iteration 9, loss = 1.17041371\n",
      "Iteration 10, loss = 1.12110589\n",
      "Iteration 11, loss = 1.07358008\n",
      "Iteration 12, loss = 1.03679092\n",
      "Iteration 13, loss = 1.00474070\n",
      "Iteration 14, loss = 0.97298690\n",
      "Iteration 15, loss = 0.94589371\n",
      "Iteration 16, loss = 0.92051278\n",
      "Iteration 17, loss = 0.89480893\n",
      "Iteration 18, loss = 0.87292932\n",
      "Iteration 19, loss = 0.84832888\n",
      "Iteration 20, loss = 0.82650054\n",
      "Iteration 21, loss = 0.80645614\n",
      "Iteration 22, loss = 0.78835510\n",
      "Iteration 23, loss = 0.76874525\n",
      "Iteration 24, loss = 0.75254667\n",
      "Iteration 25, loss = 0.73259700\n",
      "Iteration 26, loss = 0.71952956\n",
      "Iteration 27, loss = 0.70244477\n",
      "Iteration 28, loss = 0.68857793\n",
      "Iteration 29, loss = 0.67245257\n",
      "Iteration 30, loss = 0.66097361\n",
      "Iteration 31, loss = 0.64636006\n",
      "Iteration 32, loss = 0.63455522\n",
      "Iteration 33, loss = 0.62332761\n",
      "Iteration 34, loss = 0.61314445\n",
      "Iteration 35, loss = 0.60200208\n",
      "Iteration 36, loss = 0.59365949\n",
      "Iteration 37, loss = 0.58603645\n",
      "Iteration 38, loss = 0.57395705\n",
      "Iteration 39, loss = 0.56471821\n",
      "Iteration 40, loss = 0.55661838\n",
      "Iteration 41, loss = 0.54943590\n",
      "Iteration 42, loss = 0.54118233\n",
      "Iteration 43, loss = 0.53311911\n",
      "Iteration 44, loss = 0.52634641\n",
      "Iteration 45, loss = 0.52357651\n",
      "Iteration 46, loss = 0.51608916\n",
      "Iteration 47, loss = 0.50697203\n",
      "Iteration 48, loss = 0.50110485\n",
      "Iteration 49, loss = 0.49573385\n",
      "Iteration 50, loss = 0.48958652\n",
      "Iteration 51, loss = 0.48114664\n",
      "Iteration 52, loss = 0.47741165\n",
      "Iteration 53, loss = 0.46510576\n",
      "Iteration 54, loss = 0.46048672\n",
      "Iteration 55, loss = 0.45040712\n",
      "Iteration 56, loss = 0.44371987\n",
      "Iteration 57, loss = 0.43589618\n",
      "Iteration 58, loss = 0.43091133\n",
      "Iteration 59, loss = 0.42159497\n",
      "Iteration 60, loss = 0.41455269\n",
      "Iteration 61, loss = 0.40858428\n",
      "Iteration 62, loss = 0.40152708\n",
      "Iteration 63, loss = 0.39565112\n",
      "Iteration 64, loss = 0.38891406\n",
      "Iteration 65, loss = 0.38314380\n",
      "Iteration 66, loss = 0.37798783\n",
      "Iteration 67, loss = 0.37205915\n",
      "Iteration 68, loss = 0.36731584\n",
      "Iteration 69, loss = 0.36172866\n",
      "Iteration 70, loss = 0.35600670\n",
      "Iteration 71, loss = 0.35055225\n",
      "Iteration 72, loss = 0.34615675\n",
      "Iteration 73, loss = 0.34143502\n",
      "Iteration 74, loss = 0.33589246\n",
      "Iteration 75, loss = 0.33253429\n",
      "Iteration 76, loss = 0.32701748\n",
      "Iteration 77, loss = 0.32195726\n",
      "Iteration 78, loss = 0.31894041\n",
      "Iteration 79, loss = 0.31385268\n",
      "Iteration 80, loss = 0.31002660\n",
      "Iteration 81, loss = 0.30643779\n",
      "Iteration 82, loss = 0.30218609\n",
      "Iteration 83, loss = 0.29715981\n",
      "Iteration 84, loss = 0.29543140\n",
      "Iteration 85, loss = 0.29061314\n",
      "Iteration 86, loss = 0.28954316\n",
      "Iteration 87, loss = 0.28586530\n",
      "Iteration 88, loss = 0.27882506\n",
      "Iteration 89, loss = 0.27790751\n",
      "Iteration 90, loss = 0.27268603\n",
      "Iteration 91, loss = 0.27053940\n",
      "Iteration 92, loss = 0.26629795\n",
      "Iteration 93, loss = 0.26412826\n",
      "Iteration 94, loss = 0.26324762\n",
      "Iteration 95, loss = 0.25607121\n",
      "Iteration 96, loss = 0.25705462\n",
      "Iteration 97, loss = 0.25020809\n",
      "Iteration 98, loss = 0.25001839\n",
      "Iteration 99, loss = 0.24403819\n",
      "Iteration 100, loss = 0.24379350\n",
      "Iteration 101, loss = 0.24305067\n",
      "Iteration 102, loss = 0.23614715\n",
      "Iteration 103, loss = 0.23382451\n",
      "Iteration 104, loss = 0.23075420\n",
      "Iteration 105, loss = 0.22975345\n",
      "Iteration 106, loss = 0.22583900\n",
      "Iteration 107, loss = 0.22694159\n",
      "Iteration 108, loss = 0.22680338\n",
      "Iteration 109, loss = 0.21763400\n",
      "Iteration 110, loss = 0.21740362\n",
      "Iteration 111, loss = 0.21586797\n",
      "Iteration 112, loss = 0.21190035\n",
      "Iteration 113, loss = 0.21014268\n",
      "Iteration 114, loss = 0.20762820\n",
      "Iteration 115, loss = 0.20484095\n",
      "Iteration 116, loss = 0.20269286\n",
      "Iteration 117, loss = 0.20123277\n",
      "Iteration 118, loss = 0.19833841\n",
      "Iteration 119, loss = 0.19644333\n",
      "Iteration 120, loss = 0.19567437\n",
      "Iteration 121, loss = 0.19342245\n",
      "Iteration 122, loss = 0.19173993\n",
      "Iteration 123, loss = 0.19072406\n",
      "Iteration 124, loss = 0.19032356\n",
      "Iteration 125, loss = 0.18723596\n",
      "Iteration 126, loss = 0.18458529\n",
      "Iteration 127, loss = 0.18270626\n",
      "Iteration 128, loss = 0.18171213\n",
      "Iteration 129, loss = 0.17858243\n",
      "Iteration 130, loss = 0.17747241\n",
      "Iteration 131, loss = 0.17598339\n",
      "Iteration 132, loss = 0.17367914\n",
      "Iteration 133, loss = 0.17382476\n",
      "Iteration 134, loss = 0.17009089\n",
      "Iteration 135, loss = 0.17160257\n",
      "Iteration 136, loss = 0.17062764\n",
      "Iteration 137, loss = 0.16534896\n",
      "Iteration 138, loss = 0.16446967\n",
      "Iteration 139, loss = 0.16582767\n",
      "Iteration 140, loss = 0.16111476\n",
      "Iteration 141, loss = 0.16061014\n",
      "Iteration 142, loss = 0.15994230\n",
      "Iteration 143, loss = 0.15759381\n",
      "Iteration 144, loss = 0.15594770\n",
      "Iteration 145, loss = 0.15606202\n",
      "Iteration 146, loss = 0.15450524\n",
      "Iteration 147, loss = 0.15317786\n",
      "Iteration 148, loss = 0.15146931\n",
      "Iteration 149, loss = 0.15023079\n",
      "Iteration 150, loss = 0.14920274\n",
      "Iteration 151, loss = 0.15225785\n",
      "Iteration 152, loss = 0.14625697\n",
      "Iteration 153, loss = 0.14547466\n",
      "Iteration 154, loss = 0.14421974\n",
      "Iteration 155, loss = 0.14351199\n",
      "Iteration 156, loss = 0.14291335\n",
      "Iteration 157, loss = 0.14455087\n",
      "Iteration 158, loss = 0.14157281\n",
      "Iteration 159, loss = 0.14013531\n",
      "Iteration 160, loss = 0.13834233\n",
      "Iteration 161, loss = 0.13918290\n",
      "Iteration 162, loss = 0.13758428\n",
      "Iteration 163, loss = 0.13471172\n",
      "Iteration 164, loss = 0.13488043\n",
      "Iteration 165, loss = 0.13286354\n",
      "Iteration 166, loss = 0.13289850\n",
      "Iteration 167, loss = 0.13139205\n",
      "Iteration 168, loss = 0.13143697\n",
      "Iteration 169, loss = 0.12990592\n",
      "Iteration 170, loss = 0.12916179\n",
      "Iteration 171, loss = 0.12828003\n",
      "Iteration 172, loss = 0.12725125\n",
      "Iteration 173, loss = 0.12685802\n",
      "Iteration 174, loss = 0.12657482\n",
      "Iteration 175, loss = 0.12432796\n",
      "Iteration 176, loss = 0.12389855\n",
      "Iteration 177, loss = 0.12395765\n",
      "Iteration 178, loss = 0.12293427\n",
      "Iteration 179, loss = 0.12190641\n",
      "Iteration 180, loss = 0.12044426\n",
      "Iteration 181, loss = 0.12085807\n",
      "Iteration 182, loss = 0.11999239\n",
      "Iteration 183, loss = 0.12106574\n",
      "Iteration 184, loss = 0.11784849\n",
      "Iteration 185, loss = 0.11731183\n",
      "Iteration 186, loss = 0.11679709\n",
      "Iteration 187, loss = 0.11582289\n",
      "Iteration 188, loss = 0.11505195\n",
      "Iteration 189, loss = 0.11624004\n",
      "Iteration 190, loss = 0.11365127\n",
      "Iteration 191, loss = 0.11385216\n",
      "Iteration 192, loss = 0.11362652\n",
      "Iteration 193, loss = 0.11320538\n",
      "Iteration 194, loss = 0.11127776\n",
      "Iteration 195, loss = 0.11056493\n",
      "Iteration 196, loss = 0.11019247\n",
      "Iteration 197, loss = 0.10938434\n",
      "Iteration 198, loss = 0.10907792\n",
      "Iteration 199, loss = 0.11094108\n",
      "Iteration 200, loss = 0.10911017\n",
      "Iteration 201, loss = 0.10688460\n",
      "Iteration 202, loss = 0.10677177\n",
      "Iteration 203, loss = 0.10621839\n",
      "Iteration 204, loss = 0.10550440\n",
      "Iteration 205, loss = 0.10540597\n",
      "Iteration 206, loss = 0.10433860\n",
      "Iteration 207, loss = 0.10416424\n",
      "Iteration 208, loss = 0.10373615\n",
      "Iteration 209, loss = 0.10304358\n",
      "Iteration 210, loss = 0.10238204\n",
      "Iteration 211, loss = 0.10178099\n",
      "Iteration 212, loss = 0.10253745\n",
      "Iteration 213, loss = 0.10065243\n",
      "Iteration 214, loss = 0.10139100\n",
      "Iteration 215, loss = 0.10473503\n",
      "Iteration 216, loss = 0.10236394\n",
      "Iteration 217, loss = 0.09989665\n",
      "Iteration 218, loss = 0.09891096\n",
      "Iteration 219, loss = 0.09939591\n",
      "Iteration 220, loss = 0.10119130\n",
      "Iteration 221, loss = 0.09706190\n",
      "Iteration 222, loss = 0.09798006\n",
      "Iteration 223, loss = 0.09707054\n",
      "Iteration 224, loss = 0.09713712\n",
      "Iteration 225, loss = 0.09660875\n",
      "Iteration 226, loss = 0.09663119\n",
      "Iteration 227, loss = 0.09484372\n",
      "Iteration 228, loss = 0.09656171\n",
      "Iteration 229, loss = 0.09458237\n",
      "Iteration 230, loss = 0.09427047\n",
      "Iteration 231, loss = 0.09362427\n",
      "Iteration 232, loss = 0.09308312\n",
      "Iteration 233, loss = 0.09325394\n",
      "Iteration 234, loss = 0.09228941\n",
      "Iteration 235, loss = 0.09241620\n",
      "Iteration 236, loss = 0.09151906\n",
      "Iteration 237, loss = 0.09266828\n",
      "Iteration 238, loss = 0.08962935\n",
      "Iteration 239, loss = 0.09229031\n",
      "Iteration 240, loss = 0.08964163\n",
      "Iteration 241, loss = 0.09028065\n",
      "Iteration 242, loss = 0.08936245\n",
      "Iteration 243, loss = 0.08932396\n",
      "Iteration 244, loss = 0.08876262\n",
      "Iteration 245, loss = 0.08839279\n",
      "Iteration 246, loss = 0.08778860\n",
      "Iteration 247, loss = 0.09186589\n",
      "Iteration 248, loss = 0.08601158\n",
      "Iteration 249, loss = 0.08913588\n",
      "Iteration 250, loss = 0.08830487\n",
      "Iteration 251, loss = 0.08715911\n",
      "Iteration 252, loss = 0.08720084\n",
      "Iteration 253, loss = 0.08595909\n",
      "Iteration 254, loss = 0.08659991\n",
      "Iteration 255, loss = 0.08606938\n",
      "Iteration 256, loss = 0.08532553\n",
      "Iteration 257, loss = 0.08559611\n",
      "Iteration 258, loss = 0.08577490\n",
      "Iteration 259, loss = 0.08557039\n",
      "Iteration 260, loss = 0.08507476\n",
      "Iteration 261, loss = 0.08459227\n",
      "Iteration 262, loss = 0.08612319\n",
      "Iteration 263, loss = 0.08522679\n",
      "Iteration 264, loss = 0.08263237\n",
      "Iteration 265, loss = 0.08287833\n",
      "Iteration 266, loss = 0.08212850\n",
      "Iteration 267, loss = 0.08355807\n",
      "Iteration 268, loss = 0.08439522\n",
      "Iteration 269, loss = 0.08366551\n",
      "Iteration 270, loss = 0.08133942\n",
      "Iteration 271, loss = 0.08152976\n",
      "Iteration 272, loss = 0.08097790\n",
      "Iteration 273, loss = 0.08133977\n",
      "Iteration 274, loss = 0.08143214\n",
      "Iteration 275, loss = 0.08004292\n",
      "Iteration 276, loss = 0.08064937\n",
      "Iteration 277, loss = 0.08015252\n",
      "Iteration 278, loss = 0.07909858\n",
      "Iteration 279, loss = 0.07905402\n",
      "Iteration 280, loss = 0.07860276\n",
      "Iteration 281, loss = 0.07951599\n",
      "Iteration 282, loss = 0.07861664\n",
      "Iteration 283, loss = 0.07909591\n",
      "Iteration 284, loss = 0.07782827\n",
      "Iteration 285, loss = 0.07948777\n",
      "Iteration 286, loss = 0.07705228\n",
      "Iteration 287, loss = 0.07784224\n",
      "Iteration 288, loss = 0.07723424\n",
      "Iteration 289, loss = 0.07692231\n",
      "Iteration 290, loss = 0.07713531\n",
      "Iteration 291, loss = 0.07943792\n",
      "Iteration 292, loss = 0.07640899\n",
      "Iteration 293, loss = 0.07587269\n",
      "Iteration 294, loss = 0.07601615\n",
      "Iteration 295, loss = 0.07640203\n",
      "Iteration 296, loss = 0.07544157\n",
      "Iteration 297, loss = 0.07639115\n",
      "Iteration 298, loss = 0.07644806\n",
      "Iteration 299, loss = 0.07570033\n",
      "Iteration 300, loss = 0.07449521\n",
      "Iteration 301, loss = 0.07520592\n",
      "Iteration 302, loss = 0.07855861\n",
      "Iteration 303, loss = 0.07307318\n",
      "Iteration 304, loss = 0.07550570\n",
      "Iteration 305, loss = 0.07365299\n",
      "Iteration 306, loss = 0.07426593\n",
      "Iteration 307, loss = 0.07338815\n",
      "Iteration 308, loss = 0.07582690\n",
      "Iteration 309, loss = 0.07442961\n",
      "Iteration 310, loss = 0.07351230\n",
      "Iteration 311, loss = 0.07509967\n",
      "Iteration 312, loss = 0.07195877\n",
      "Iteration 313, loss = 0.07312217\n",
      "Iteration 314, loss = 0.07253805\n",
      "Iteration 315, loss = 0.07231421\n",
      "Iteration 316, loss = 0.07167338\n",
      "Iteration 317, loss = 0.07313583\n",
      "Iteration 318, loss = 0.07725933\n",
      "Iteration 319, loss = 0.07099868\n",
      "Iteration 320, loss = 0.07177973\n",
      "Iteration 321, loss = 0.07218251\n",
      "Iteration 322, loss = 0.07257143\n",
      "Iteration 323, loss = 0.07144441\n",
      "Iteration 324, loss = 0.07107552\n",
      "Iteration 325, loss = 0.07030424\n",
      "Iteration 326, loss = 0.07067954\n",
      "Iteration 327, loss = 0.07030101\n",
      "Iteration 328, loss = 0.07029363\n",
      "Iteration 329, loss = 0.06954314\n",
      "Iteration 330, loss = 0.07194073\n",
      "Iteration 331, loss = 0.07057397\n",
      "Iteration 332, loss = 0.07004004\n",
      "Iteration 333, loss = 0.07165578\n",
      "Iteration 334, loss = 0.06940251\n",
      "Iteration 335, loss = 0.07100749\n",
      "Iteration 336, loss = 0.06918533\n",
      "Iteration 337, loss = 0.06916999\n",
      "Iteration 338, loss = 0.07195196\n",
      "Iteration 339, loss = 0.06954407\n",
      "Iteration 340, loss = 0.06860971\n",
      "Iteration 341, loss = 0.06931413\n",
      "Iteration 342, loss = 0.07000790\n",
      "Iteration 343, loss = 0.06912624\n",
      "Iteration 344, loss = 0.06787945\n",
      "Iteration 345, loss = 0.06835903\n",
      "Iteration 346, loss = 0.06762316\n",
      "Iteration 347, loss = 0.07096147\n",
      "Iteration 348, loss = 0.06972997\n",
      "Iteration 349, loss = 0.07082944\n",
      "Iteration 350, loss = 0.06655910\n",
      "Iteration 351, loss = 0.06673553\n",
      "Iteration 352, loss = 0.06652594\n",
      "Iteration 353, loss = 0.07216762\n",
      "Iteration 354, loss = 0.07773423\n",
      "Iteration 355, loss = 0.06894745\n",
      "Iteration 356, loss = 0.06783419\n",
      "Iteration 357, loss = 0.07042252\n",
      "Iteration 358, loss = 0.06515077\n",
      "Iteration 359, loss = 0.06665738\n",
      "Iteration 360, loss = 0.06740042\n",
      "Iteration 361, loss = 0.06550308\n",
      "Iteration 362, loss = 0.06804899\n",
      "Iteration 363, loss = 0.06628030\n",
      "Iteration 364, loss = 0.06626255\n",
      "Iteration 365, loss = 0.06536681\n",
      "Iteration 366, loss = 0.06589505\n",
      "Iteration 367, loss = 0.06499296\n",
      "Iteration 368, loss = 0.06584171\n",
      "Iteration 369, loss = 0.06493724\n",
      "Iteration 370, loss = 0.06550307\n",
      "Iteration 371, loss = 0.06438796\n",
      "Iteration 372, loss = 0.06482084\n",
      "Iteration 373, loss = 0.06550288\n",
      "Iteration 374, loss = 0.06473478\n",
      "Iteration 375, loss = 0.06422624\n",
      "Iteration 376, loss = 0.06388299\n",
      "Iteration 377, loss = 0.06554845\n",
      "Iteration 378, loss = 0.06412073\n",
      "Iteration 379, loss = 0.06499361\n",
      "Iteration 380, loss = 0.06603191\n",
      "Iteration 381, loss = 0.06288747\n",
      "Iteration 382, loss = 0.06412769\n",
      "Iteration 383, loss = 0.06334155\n",
      "Iteration 384, loss = 0.06438060\n",
      "Iteration 385, loss = 0.06410483\n",
      "Iteration 386, loss = 0.06428074\n",
      "Iteration 387, loss = 0.06772743\n",
      "Iteration 388, loss = 0.06186621\n",
      "Iteration 389, loss = 0.06461414\n",
      "Iteration 390, loss = 0.06326988\n",
      "Iteration 391, loss = 0.06418450\n",
      "Iteration 392, loss = 0.06601499\n",
      "Iteration 393, loss = 0.06215444\n",
      "Iteration 394, loss = 0.06228414\n",
      "Iteration 395, loss = 0.06319524\n",
      "Iteration 396, loss = 0.06218236\n",
      "Iteration 397, loss = 0.06412507\n",
      "Iteration 398, loss = 0.06246482\n",
      "Iteration 399, loss = 0.06240682\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Trainingsergebnis: 0.992\n",
      "[[ 7  0  0]\n",
      " [ 0 11  0]\n",
      " [ 0  1 11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00         7\n",
      "         1.0       0.92      1.00      0.96        11\n",
      "         2.0       1.00      0.92      0.96        12\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.97      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n",
      "Testergebnis: 0.967\n",
      "Weights: [array([[-3.52275583e-002,  3.90391788e-001, -5.87841453e-002,\n",
      "         3.69769958e-001,  8.16174284e-002, -2.17123801e-080,\n",
      "         5.66854986e-001,  3.10424207e-001,  3.92748137e-001,\n",
      "        -2.99798151e-002],\n",
      "       [ 6.66709715e-001, -3.21681222e-001, -8.76307037e-002,\n",
      "         5.30576848e-001, -6.15242392e-001, -2.15696805e-112,\n",
      "         3.65282052e-001,  6.47567192e-001,  4.00986114e-001,\n",
      "        -3.78034057e-001],\n",
      "       [ 5.75483849e-001,  6.51865012e-001, -5.79647199e-002,\n",
      "        -5.59509619e-001,  1.45219579e+000, -3.21952242e-034,\n",
      "        -1.56652573e-002,  5.13245755e-001,  2.33875717e-002,\n",
      "         5.63104198e-002],\n",
      "       [ 3.81736836e-001,  5.80886502e-001, -2.08476221e-002,\n",
      "        -6.83356931e-001,  1.23574494e+000,  6.26344643e-032,\n",
      "        -1.00876691e+000, -2.03304879e-001,  2.27510964e-001,\n",
      "         1.09998025e+000]]), array([[ 4.91276651e-001, -4.93586887e-001,  4.52835556e-001],\n",
      "       [-8.75008798e-002, -5.93595521e-002,  3.13360224e-001],\n",
      "       [ 2.15827442e-002, -7.18686557e-003, -4.67654431e-002],\n",
      "       [ 2.30797908e+000,  1.17785753e+000, -2.36236305e+000],\n",
      "       [-2.12904764e+000,  4.94103186e-002,  7.69818977e-001],\n",
      "       [-1.60855168e-035,  3.98388650e-043,  2.45855297e-107],\n",
      "       [ 8.02960644e-001,  3.54650508e-001, -6.57732537e-001],\n",
      "       [-2.49600369e-001,  1.42989607e-001, -7.06813826e-001],\n",
      "       [ 1.07332539e-001,  1.19449661e-001,  2.70488195e-001],\n",
      "       [-3.40487767e-001, -1.02206806e+000,  9.72946882e-001]])]\n",
      "BIASES: [array([-0.49764573,  0.13994542,  0.4377293 ,  1.19938229, -0.83746163,\n",
      "       -0.19448985,  0.01209831,  0.69629561, -0.54738561, -0.3477098 ]), array([-0.0452075 , -0.22606495, -1.07104252])]\n",
      "[0. 2. 0. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Laden Sie das Iris-Datenset\n",
    "data_train = pd.read_csv('iris.csv')\n",
    "\n",
    "# Die 3 zu erkennen Klassifikationsklassen werden in die numerischen Werte\n",
    "# 0, 1 bzw. 2 umgewandelt.\n",
    "data_train.loc[data_train['species']=='Iris-setosa', 'species']=0\n",
    "data_train.loc[data_train['species']=='Iris-versicolor', 'species']=1\n",
    "data_train.loc[data_train['species']=='Iris-virginica', 'species']=2\n",
    "data_train = data_train.apply(pd.to_numeric)\n",
    "\n",
    "# Das eingelesene Datenset wird als Matrix dargestellt\n",
    "data_train_array = data_train.values\n",
    "\n",
    "# Zur Sicherstellung der Reproduzierbarkeit der Ergebnisse setzen wir\n",
    "# random.seed auf einen festen Wert, z. B. 17\n",
    "np.random.seed(17)\n",
    "\n",
    "# Trainingsdaten: 80%\n",
    "# Testdaten: 20%\n",
    "# X ist ein Vektor\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_train_array[:, :4],\n",
    "                                                    data_train_array[:, 4],\n",
    "                                                    test_size=0.2)\n",
    "\n",
    "# Variante 1\n",
    "# MultilayerPerceptron\n",
    "# ein Input Layer mit 4 Neuronen für die Merkmale der Pflanzen\n",
    "# ein Hidden-Layer mit 10 Neuronen\n",
    "# ein Output Layer, die die zu erkennenden Klassen repräsentieren\n",
    "# Aktivierungsfunktion: relu\n",
    "# Optimierer: adam\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,), activation='relu', solver='adam', max_iter=400, batch_size=10, verbose=True)\n",
    "\n",
    "# Variante 2\n",
    "# 2 Hiddenlayer mit 5 bzw. 3 Neuronen\n",
    "# Aktivierungsfunktion: tahn\n",
    "# Optimierer: adam\n",
    "#mlp = MLPClassifier(hidden_layer_sizes=(5,3), activation='tahn', solver='adam', max_iter=350, batch_size=10, verbose=True)\n",
    "\n",
    "# Trainieren\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Ergebnis\n",
    "print(\"Trainingsergebnis: %5.3f\" % mlp.score(X_train, y_train))\n",
    "\n",
    "# Das Modell mit den Testdaten evaluieren\n",
    "predictions = mlp.predict(X_test)\n",
    "# und die Konfusionsmatrix wird ausgegeben\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "\n",
    "# Score berechnen und ausgeben\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# Testen des Modells\n",
    "print(\"Testergebnis: %5.3f\" % mlp.score(X_test, y_test))\n",
    "\n",
    "# Gewichtung ausgeben\n",
    "print(\"Weights:\", mlp.coefs_)\n",
    "print(\"BIASES:\", mlp.intercepts_)\n",
    "\n",
    "# Das Modell wird beispielsweise zur Vorhersage auf folgenden Werten\n",
    "# aus dem Testset angewandt mit den Merkmalen [sepal-length, sepal-width,\n",
    "# petal-length, petal-width]\n",
    "print(mlp.predict([[5.1,3.5,1.4,0.2], [5.9,3.,5.1,1.8], [4.9,3.,1.4,0.2], [5.8,2.7,4.1,1.]]))\n",
    "\n",
    "# Die Loss-Kurve wird visualisiert und in der Datei Plot_of_loss_values.png im PNG-Format gespeichert.\n",
    "loss_values = mlp.loss_curve_\n",
    "plt.plot(loss_values)\n",
    "plt.savefig(\"./Plot_of_loss_values.png\")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}